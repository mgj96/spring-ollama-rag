server:
  port: 8080

logging:
  level:
    root: INFO

spring:
  main:
    banner-mode: off
  ai:
    ollama:
      base-url: http://localhost:11434
      chat:
        options:
          model: llama3.2
    mcp:
      server:
        protocol: STREAMABLE
        streamable-http:
          mcp-endpoint: /mcp

management:
  endpoints:
    web:
      exposure:
        include: health, info, env
  endpoint:
    health:
      show-details: always
  info:
    env:
      enabled: true
